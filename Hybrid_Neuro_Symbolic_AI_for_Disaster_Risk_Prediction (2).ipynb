{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2r_YxvXj50t"
      },
      "source": [
        "##**Hybrid Neuro-Symbolic Deep Learning for Tropical Cyclone Intensity Prediction and GIS-Based Risk Visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt-mRw0t0amK"
      },
      "source": [
        "**Problem Statement:**\n",
        "\n",
        "Traditional neural models for tropical cyclone intensity prediction can achieve good accuracy but may produce physically implausible forecasts (for example, negative wind speeds or unrealistic intensity jumps) and behave unreliably during extreme events. The goal of this project is to develop a hybrid neuro-symbolic model that predicts short-term cyclone intensity from historical track sequences (latitude, longitude, pressure, wind) while incorporating soft physical and domain constraints as differentiable penalties. This approach aims to improve both predictive performance and physical consistency compared to a neural-only baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M70pedEDdAF"
      },
      "source": [
        "##**Install + imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFJBsb5_CmqS"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRZ1C3qVDjHA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btGwKZB-EDC7"
      },
      "source": [
        "What this block does:\n",
        "\n",
        "Installs and imports everything we need: data handling, scaling, train split, and PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwhSRtf6ESA1"
      },
      "source": [
        "##**Parse latitude/longitude strings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKzp7unDEAja"
      },
      "outputs": [],
      "source": [
        "def parse_latlon(s: str) -> float:\n",
        "    s = str(s).strip()\n",
        "    if not s:\n",
        "        return np.nan\n",
        "    hemi = s[-1].upper()       # N/S/E/W\n",
        "    val = float(s[:-1])        # numeric part\n",
        "    if hemi in (\"S\", \"W\"):\n",
        "        return -val\n",
        "    return val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t9lRxvHEa6F"
      },
      "source": [
        "What it does:\n",
        "\n",
        "Converts \"28.0N\" → 28.0\n",
        "\n",
        "Converts \"94.8W\" → -94.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WnDFqaNEgqe"
      },
      "source": [
        "##**Distance between two track points (Haversine)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBd8Iz3LEnCQ"
      },
      "source": [
        "We use this for a “sanity rule” (storms usually don’t teleport hundreds of km in 6 hours)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPfVpE-2EYnD"
      },
      "outputs": [],
      "source": [
        "def haversine_km(lat1, lon1, lat2, lon2) -> float:\n",
        "    R = 6371.0\n",
        "    p1, p2 = math.radians(lat1), math.radians(lat2)\n",
        "    dphi = math.radians(lat2 - lat1)\n",
        "    dlmb = math.radians(lon2 - lon1)\n",
        "    a = math.sin(dphi/2)**2 + math.cos(p1)*math.cos(p2)*math.sin(dlmb/2)**2\n",
        "    return 2 * R * math.asin(min(1.0, math.sqrt(a)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SINZhS1yEuqB"
      },
      "source": [
        "##**Load your atlantic + pacific CSVs and clean**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VYX3TkrEqIX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_tracks(atl_path: str, pac_path: str) -> pd.DataFrame:\n",
        "    atl = pd.read_csv(\"/content/atlantic.csv\")\n",
        "    # Use on_bad_lines='skip' to ignore rows that cause parsing errors\n",
        "    pac = pd.read_csv(\"/content/pacific.csv\", on_bad_lines='skip')\n",
        "\n",
        "    atl[\"BASIN\"] = \"ATL\"\n",
        "    pac[\"BASIN\"] = \"PAC\"\n",
        "    df = pd.concat([atl, pac], ignore_index=True)\n",
        "\n",
        "    # --- Clean string columns (optional but helps) ---\n",
        "    for c in [\"ID\", \"Name\", \"Event\", \"Status\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].astype(str).str.strip()\n",
        "\n",
        "    # --- Replace HURDAT-style missing values ---\n",
        "    # Your files use -999 for missing radii/pressure etc.\n",
        "    df = df.replace(-999, np.nan)\n",
        "\n",
        "    # --- Parse coordinates safely ---\n",
        "    def _parse_coord(v):\n",
        "        # if already numeric, return as float\n",
        "        if pd.isna(v):\n",
        "            return np.nan\n",
        "        if isinstance(v, (int, float, np.integer, np.floating)):\n",
        "            return float(v)\n",
        "        v = str(v).strip()\n",
        "        return parse_latlon(v)  # uses your parse_latlon(\"28.0N\") -> 28.0 etc.\n",
        "\n",
        "    df[\"lat\"] = df[\"Latitude\"].apply(_parse_coord)\n",
        "    df[\"lon\"] = df[\"Longitude\"].apply(_parse_coord)\n",
        "\n",
        "    # --- Make Date/Time numeric safely ---\n",
        "    df[\"Date\"] = pd.to_numeric(df[\"Date\"], errors=\"coerce\")\n",
        "    df[\"Time\"] = pd.to_numeric(df[\"Time\"], errors=\"coerce\")\n",
        "\n",
        "    # Create sortable key: YYYYMMDDHHMM (Time is 0/600/1200/1800 etc.)\n",
        "    df[\"dt_key\"] = df[\"Date\"] * 10000 + df[\"Time\"]\n",
        "\n",
        "    # --- Ensure target is numeric and filter usable rows ---\n",
        "    df[\"Maximum Wind\"] = pd.to_numeric(df[\"Maximum Wind\"], errors=\"coerce\")\n",
        "\n",
        "    # Keep only rows usable for training\n",
        "    df = df.dropna(subset=[\"ID\", \"dt_key\", \"lat\", \"lon\", \"Maximum Wind\"]).reset_index(drop=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ7S4mSNE7ef"
      },
      "outputs": [],
      "source": [
        "df = load_tracks(\"/content/atlantic.csv\", \"/content/pacific.csv\")\n",
        "print(df.columns)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz65Sq_tkPKZ"
      },
      "source": [
        "##**Build sequences (examples) per storm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSeRZSDGkUf4"
      },
      "source": [
        "Goal:\n",
        "\n",
        "Input: last T=6 points\n",
        "\n",
        "Output: next-step wind (regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUKD1BOmE8ba"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "\n",
        "@dataclass\n",
        "class SeqExample:\n",
        "    x: np.ndarray   # shape: (T, F)\n",
        "    y: float        # next-step Maximum Wind\n",
        "    meta: dict      # extra info for symbolic rules / debugging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDF8DCeKkzJ6"
      },
      "source": [
        "Build vocabularies for categorical fields:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gu11eGak04H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import Dict\n",
        "\n",
        "def make_vocab(series: pd.Series) -> Dict[str, int]:\n",
        "    series = series.fillna(\"UNK\").astype(str).str.strip()\n",
        "    uniq = sorted(series.unique().tolist())\n",
        "    return {s: i for i, s in enumerate(uniq)}\n",
        "\n",
        "def one_hot(idx: int, n: int) -> np.ndarray:\n",
        "    v = np.zeros(n, dtype=np.float32)\n",
        "    v[idx] = 1.0\n",
        "    return v\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hle90FRvk-FT"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def haversine_km(lat1, lon1, lat2, lon2) -> float:\n",
        "    R = 6371.0\n",
        "    p1, p2 = math.radians(lat1), math.radians(lat2)\n",
        "    dphi = math.radians(lat2 - lat1)\n",
        "    dlmb = math.radians(lon2 - lon1)\n",
        "    a = math.sin(dphi/2)**2 + math.cos(p1)*math.cos(p2)*math.sin(dlmb/2)**2\n",
        "    return 2 * R * math.asin(min(1.0, math.sqrt(a)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHltACI7lCeT"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "def build_examples(df: pd.DataFrame, seq_len: int = 6) -> Tuple[List[SeqExample], List[str]]:\n",
        "    df = df.copy()\n",
        "\n",
        "    # Required columns\n",
        "    required = [\"ID\", \"dt_key\", \"lat\", \"lon\", \"Maximum Wind\", \"BASIN\"]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "    # Clean strings + missing markers\n",
        "    df = df.replace(-999, np.nan)\n",
        "    df[\"BASIN\"] = df[\"BASIN\"].fillna(\"UNK\").astype(str).str.strip()\n",
        "    if \"Status\" in df.columns:\n",
        "        df[\"Status\"] = df[\"Status\"].fillna(\"UNK\").astype(str).str.strip()\n",
        "    else:\n",
        "        df[\"Status\"] = \"UNK\"\n",
        "\n",
        "    # Coerce to numeric safely\n",
        "    df[\"Maximum Wind\"] = pd.to_numeric(df[\"Maximum Wind\"], errors=\"coerce\")\n",
        "    df[\"Minimum Pressure\"] = pd.to_numeric(df.get(\"Minimum Pressure\", 0.0), errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    # Wind radii columns (optional)\n",
        "    wind_radius_cols = [\n",
        "        \"Low Wind NE\",\"Low Wind SE\",\"Low Wind SW\",\"Low Wind NW\",\n",
        "        \"Moderate Wind NE\",\"Moderate Wind SE\",\"Moderate Wind SW\",\"Moderate Wind NW\",\n",
        "        \"High Wind NE\",\"High Wind SE\",\"High Wind SW\",\"High Wind NW\"\n",
        "    ]\n",
        "    for c in wind_radius_cols:\n",
        "        if c not in df.columns:\n",
        "            df[c] = 0.0\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    # Vocabularies\n",
        "    status_vocab = make_vocab(df[\"Status\"])\n",
        "    basin_vocab = {\"ATL\": 0, \"PAC\": 1, \"UNK\": 2}\n",
        "\n",
        "    # Features per timestep (include current wind)\n",
        "    numeric_in_cols = [\"lat\", \"lon\", \"Minimum Pressure\"] + wind_radius_cols + [\"Maximum Wind\"]\n",
        "\n",
        "    feature_names = (\n",
        "        numeric_in_cols\n",
        "        + [f\"basin_{k}\" for k in basin_vocab.keys()]\n",
        "        + [f\"status_{k}\" for k in status_vocab.keys()]\n",
        "    )\n",
        "\n",
        "    examples: List[SeqExample] = []\n",
        "\n",
        "    for storm_id, g in df.groupby(\"ID\"):\n",
        "        # sort + remove duplicate timestamps\n",
        "        g = g.sort_values(\"dt_key\").drop_duplicates(\"dt_key\").reset_index(drop=True)\n",
        "\n",
        "        # Need seq_len history + 1 future point\n",
        "        if len(g) < seq_len + 1:\n",
        "            continue\n",
        "\n",
        "        # Precompute step distances (km)\n",
        "        lats = g[\"lat\"].to_numpy(dtype=np.float32)\n",
        "        lons = g[\"lon\"].to_numpy(dtype=np.float32)\n",
        "        step_km = np.zeros(len(g), dtype=np.float32)\n",
        "        for i in range(1, len(g)):\n",
        "            step_km[i] = haversine_km(float(lats[i-1]), float(lons[i-1]),\n",
        "                                     float(lats[i]), float(lons[i]))\n",
        "\n",
        "        # Sliding window: predict wind at t+1 from [t-seq_len+1 .. t]\n",
        "        for t in range(seq_len - 1, len(g) - 1):\n",
        "            hist = g.iloc[t-(seq_len-1):t+1]   # seq_len rows\n",
        "            nxt = g.iloc[t+1]\n",
        "\n",
        "            # Basin one-hot (from last timestep in hist)\n",
        "            basin_key = str(hist[\"BASIN\"].iloc[-1])\n",
        "            if basin_key not in basin_vocab:\n",
        "                basin_key = \"UNK\"\n",
        "            basin_oh = one_hot(basin_vocab[basin_key], len(basin_vocab))\n",
        "\n",
        "            X = []\n",
        "            for i in range(seq_len):\n",
        "                row = hist.iloc[i]\n",
        "\n",
        "                num = row[numeric_in_cols].to_numpy(dtype=np.float32)\n",
        "\n",
        "                st = str(row[\"Status\"])\n",
        "                if st not in status_vocab:\n",
        "                    st = \"UNK\"\n",
        "                st_oh = one_hot(status_vocab[st], len(status_vocab))\n",
        "\n",
        "                feat = np.concatenate([num, basin_oh, st_oh]).astype(np.float32)\n",
        "                X.append(feat)\n",
        "\n",
        "            X = np.stack(X, axis=0)  # (T, F)\n",
        "            y = float(nxt[\"Maximum Wind\"])\n",
        "\n",
        "            if not np.isfinite(y):\n",
        "                continue\n",
        "\n",
        "            meta = {\n",
        "                \"storm_id\": storm_id,\n",
        "                \"pressure_hist\": hist[\"Minimum Pressure\"].to_numpy(dtype=np.float32),\n",
        "                \"step_km_hist\": step_km[t-(seq_len-1):t+1].copy(),\n",
        "            }\n",
        "\n",
        "            examples.append(SeqExample(x=X, y=y, meta=meta))\n",
        "\n",
        "    return examples, feature_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEx1w2dhlMaF"
      },
      "outputs": [],
      "source": [
        "examples, feature_names = build_examples(df, seq_len=6)\n",
        "print(\"Examples:\", len(examples))\n",
        "print(\"Num features:\", len(feature_names))\n",
        "\n",
        "if examples:\n",
        "    print(\"X shape:\", examples[0].x.shape, \"y:\", examples[0].y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSzaWl7gv6_b"
      },
      "source": [
        "##**Block 5: Dataset + scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6a6WTrgvpk5"
      },
      "source": [
        "**Block 5: Dataset class (with optional scaler)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1xVtVXynxVz"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "\n",
        "class HurricaneSeqDataset(Dataset):\n",
        "    def __init__(self, examples: List[SeqExample], scaler: Optional[StandardScaler] = None, fit_scaler: bool = False):\n",
        "        \"\"\"\n",
        "        examples: list of SeqExample where ex.x is (T,F)\n",
        "        scaler: StandardScaler instance to apply\n",
        "        fit_scaler: if True, fit scaler using these examples (use ONLY on training set)\n",
        "        \"\"\"\n",
        "        self.examples = examples\n",
        "        self.scaler = scaler\n",
        "\n",
        "        if self.scaler is not None and fit_scaler:\n",
        "            # Fit on ALL timesteps from ALL sequences in training set\n",
        "            X_all = np.concatenate([ex.x.reshape(-1, ex.x.shape[-1]) for ex in examples], axis=0)\n",
        "            self.scaler.fit(X_all)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx]\n",
        "        x = ex.x  # (T, F)\n",
        "\n",
        "        if self.scaler is not None:\n",
        "            T, F = x.shape\n",
        "            x = self.scaler.transform(x.reshape(-1, F)).reshape(T, F).astype(np.float32)\n",
        "\n",
        "        y = np.float32(ex.y)\n",
        "\n",
        "        return torch.from_numpy(x), torch.tensor(y), ex.meta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHHGtH1ivw2G"
      },
      "source": [
        "**Block 5B: Collate function (keeps metas as list of dicts)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA91lX5Hvutz"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "    xs, ys, metas = zip(*batch)   # tuples length B\n",
        "    xs = torch.stack(xs, dim=0)   # (B, T, F)\n",
        "    ys = torch.stack(ys, dim=0)   # (B,)\n",
        "    return xs, ys, list(metas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_47k80IwFYZ"
      },
      "source": [
        "**Block 5C: Train/val split + scaler fit on train only**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIn-4v1YwDX2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Split indices\n",
        "idx = np.arange(len(examples))\n",
        "train_idx, val_idx = train_test_split(idx, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "train_examples = [examples[i] for i in train_idx]\n",
        "val_examples   = [examples[i] for i in val_idx]\n",
        "\n",
        "# Create scaler and FIT only on training data\n",
        "scaler = StandardScaler()\n",
        "train_ds = HurricaneSeqDataset(train_examples, scaler=scaler, fit_scaler=True)\n",
        "\n",
        "# Reuse the same fitted scaler for val\n",
        "val_ds = HurricaneSeqDataset(val_examples, scaler=scaler, fit_scaler=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAd6CcQRwK7k"
      },
      "source": [
        "**Block 5D: DataLoaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzUstXo-wIx9"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_batch\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBrbCqTQwQRo"
      },
      "source": [
        "**Block 5E: Quick sanity check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyEWb14mwOGf"
      },
      "outputs": [],
      "source": [
        "x_batch, y_batch, metas = next(iter(train_loader))\n",
        "print(\"x_batch:\", x_batch.shape)   # (B, T, F)\n",
        "print(\"y_batch:\", y_batch.shape)   # (B,)\n",
        "print(\"meta keys:\", metas[0].keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY17_2JqwVzV"
      },
      "source": [
        "That’s Block 5 done.\n",
        "\n",
        "If you want, I can also show a variant where:\n",
        "\n",
        "1. numeric features are scaled, but one-hot (basin/status) are left unscaled, which some people prefer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQtUKuyhyiZX"
      },
      "source": [
        "##**Block 6: Neural model (GRU)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rdGfWBtwTDo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GRURegressor(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 128, num_layers: int = 1, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, T, F)\n",
        "        out, _ = self.gru(x)          # out: (B, T, H)\n",
        "        h_last = out[:, -1, :]        # (B, H)\n",
        "        y_pred = self.head(h_last).squeeze(-1)  # (B,)\n",
        "        return y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaibYwweyqw_"
      },
      "source": [
        "##**Block 7: Symbolic rules (soft constraints)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1UZjjjQyxPa"
      },
      "source": [
        "Uses the meta you already create in build_examples():\n",
        "\n",
        "pressure_hist\n",
        "\n",
        "step_km_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdXXZWtcyoK8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def symbolic_penalty(\n",
        "    y_pred: torch.Tensor,   # (B,)\n",
        "    metas: list,            # list of dicts, length B\n",
        "    lambda_nonneg: float = 0.01,\n",
        "    lambda_pressure: float = 0.05,\n",
        "    lambda_motion: float = 0.02,\n",
        ") -> torch.Tensor:\n",
        "\n",
        "    device = y_pred.device\n",
        "\n",
        "    # Rule 1: wind should be non-negative\n",
        "    nonneg = torch.relu(-y_pred).mean()\n",
        "\n",
        "    # Rule 2: low pressure should not imply extremely low wind (rough heuristic baseline)\n",
        "    a, b = 200.0, 0.12\n",
        "    baseline_list = []\n",
        "    for m in metas:\n",
        "        p_last = float(m[\"pressure_hist\"][-1]) if \"pressure_hist\" in m else 0.0\n",
        "        if (not np.isfinite(p_last)) or p_last <= 0:\n",
        "            base = 0.0\n",
        "        else:\n",
        "            base = max(0.0, a - b * p_last)\n",
        "        baseline_list.append(base)\n",
        "\n",
        "    baseline = torch.tensor(baseline_list, dtype=torch.float32, device=device)\n",
        "    pressure_pen = torch.relu((baseline - y_pred) - 5.0).mean()   # 5 kt slack\n",
        "\n",
        "    # Rule 3: motion sanity (penalize big jumps in history)\n",
        "    motion_viol = []\n",
        "    for m in metas:\n",
        "        step = m.get(\"step_km_hist\", [])\n",
        "        mx = float(np.max(step)) if len(step) else 0.0\n",
        "        motion_viol.append(max(0.0, mx - 400.0))  # km threshold\n",
        "\n",
        "    motion_pen = torch.tensor(motion_viol, dtype=torch.float32, device=device).mean() / 400.0\n",
        "\n",
        "    return (\n",
        "        lambda_nonneg * nonneg\n",
        "        + lambda_pressure * pressure_pen\n",
        "        + lambda_motion * motion_pen\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6CDuTkQy4gg"
      },
      "source": [
        "##**Block 8: Training loop (neural loss + symbolic loss)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3WwqjP4y0tX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs: int = 8,\n",
        "    lr: float = 1e-3,\n",
        "    lambda_sym: float = 1.0,\n",
        "    device: str | None = None\n",
        "):\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model.to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.SmoothL1Loss()\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        # ---- Train ----\n",
        "        model.train()\n",
        "        tr_total, tr_neural, tr_sym = [], [], []\n",
        "\n",
        "        for x, y, metas in train_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            y_pred = model(x)\n",
        "\n",
        "            neural_loss = loss_fn(y_pred, y)\n",
        "            sym_loss = symbolic_penalty(y_pred, metas)\n",
        "            loss = neural_loss + lambda_sym * sym_loss\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            tr_total.append(loss.item())\n",
        "            tr_neural.append(neural_loss.item())\n",
        "            tr_sym.append(sym_loss.item())\n",
        "\n",
        "        # ---- Val ----\n",
        "        model.eval()\n",
        "        va_total, va_neural, va_sym = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y, metas in val_loader:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "\n",
        "                y_pred = model(x)\n",
        "\n",
        "                neural_loss = loss_fn(y_pred, y)\n",
        "                sym_loss = symbolic_penalty(y_pred, metas)\n",
        "                loss = neural_loss + lambda_sym * sym_loss\n",
        "\n",
        "                va_total.append(loss.item())\n",
        "                va_neural.append(neural_loss.item())\n",
        "                va_sym.append(sym_loss.item())\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {ep:02d} | \"\n",
        "            f\"train={np.mean(tr_total):.4f} (neural={np.mean(tr_neural):.4f}, sym={np.mean(tr_sym):.4f}) | \"\n",
        "            f\"val={np.mean(va_total):.4f} (neural={np.mean(va_neural):.4f}, sym={np.mean(va_sym):.4f})\"\n",
        "        )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hi6SebAzA7Z"
      },
      "source": [
        "Connect them (same style as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9enJdwOfy-kI"
      },
      "outputs": [],
      "source": [
        "input_dim = examples[0].x.shape[-1]\n",
        "model = GRURegressor(input_dim=input_dim, hidden_dim=128)\n",
        "\n",
        "model = train_model(\n",
        "    model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=8,\n",
        "    lr=1e-3,\n",
        "    lambda_sym=1.0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZOAzzEm2C_w"
      },
      "source": [
        "##**Block 9: Run everything (end-to-end)t**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtam67ij2Gyl"
      },
      "source": [
        "**Block 9A: Build examples (from cleaned df)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX0GjLBPzFFF"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN = 6\n",
        "\n",
        "examples, feature_names = build_examples(df, seq_len=SEQ_LEN)\n",
        "\n",
        "print(\"Examples:\", len(examples))\n",
        "print(\"Features per timestep:\", len(feature_names))\n",
        "\n",
        "if len(examples) == 0:\n",
        "    raise RuntimeError(\"No training examples created. Fix data preprocessing first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpsPz4IZ9vxT"
      },
      "source": [
        "**Block 9B: Train / validation split + loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YhGgPF52Kck"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "idx = np.arange(len(examples))\n",
        "train_idx, val_idx = train_test_split(idx, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "train_examples = [examples[i] for i in train_idx]\n",
        "val_examples   = [examples[i] for i in val_idx]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_ds = HurricaneSeqDataset(\n",
        "    train_examples,\n",
        "    scaler=scaler,\n",
        "    fit_scaler=True\n",
        ")\n",
        "\n",
        "val_ds = HurricaneSeqDataset(\n",
        "    val_examples,\n",
        "    scaler=scaler,\n",
        "    fit_scaler=False\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1NjMoCG-qVs"
      },
      "source": [
        "**Block 9C: Initialize model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNwouL5m92ct"
      },
      "outputs": [],
      "source": [
        "input_dim = train_examples[0].x.shape[-1]\n",
        "\n",
        "model = GRURegressor(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dim=128,\n",
        "    num_layers=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLqrL3ys-vau"
      },
      "source": [
        "**Block 9D: Train model (neural + symbolic)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gTZwIP4-taz"
      },
      "outputs": [],
      "source": [
        "model = train_model(\n",
        "    model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=8,\n",
        "    lr=1e-3,\n",
        "    lambda_sym=1.0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tJdburt_Y57"
      },
      "source": [
        "**Block 9E: Quick sanity prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADvpfDKo-yjS"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "x_batch, y_batch, metas = next(iter(val_loader))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model(x_batch.to(device)).cpu().numpy()\n",
        "\n",
        "print(\"True wind (first 10):\", y_batch[:10].numpy())\n",
        "print(\"Pred wind (first 10):\", preds[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNyZd_m4HpXT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "y_true = y_batch.cpu().numpy() if hasattr(y_batch, \"cpu\") else np.array(y_batch)\n",
        "y_pred = np.array(preds)\n",
        "\n",
        "mae = np.mean(np.abs(y_pred - y_true))\n",
        "rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
        "\n",
        "# R²\n",
        "ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "r2 = 1 - ss_res / ss_tot\n",
        "\n",
        "print(f\"MAE  : {mae:.2f} kt\")\n",
        "print(f\"RMSE : {rmse:.2f} kt\")\n",
        "print(f\"R²   : {r2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbcg22OHHxFK"
      },
      "source": [
        "**Block B: Accuracy within ±X knots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T8U0bfuHzzt"
      },
      "outputs": [],
      "source": [
        "def accuracy_within(y_true, y_pred, tol):\n",
        "    return np.mean(np.abs(y_pred - y_true) <= tol)\n",
        "\n",
        "for tol in [5, 10, 15]:\n",
        "    acc = accuracy_within(y_true, y_pred, tol)\n",
        "    print(f\"Accuracy ±{tol} kt: {acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_SEFUWDH4Bd"
      },
      "outputs": [],
      "source": [
        "def wind_to_category(w):\n",
        "    if w < 34: return 0      # TD\n",
        "    if w < 64: return 1      # TS\n",
        "    if w < 83: return 2      # Cat 1\n",
        "    if w < 96: return 3      # Cat 2\n",
        "    if w < 113: return 4     # Cat 3\n",
        "    if w < 137: return 5     # Cat 4\n",
        "    return 6                 # Cat 5\n",
        "\n",
        "y_true_cat = np.array([wind_to_category(w) for w in y_true])\n",
        "y_pred_cat = np.array([wind_to_category(w) for w in y_pred])\n",
        "\n",
        "cat_acc = np.mean(y_true_cat == y_pred_cat)\n",
        "print(f\"Category accuracy: {cat_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RjH4hzkEZm3"
      },
      "source": [
        "##**Graphs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scPRiwhiELcY"
      },
      "source": [
        "**True vs Predicted (scatter)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDmzCgvY_ffG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "y_true = y_batch.cpu().numpy() if hasattr(y_batch, \"cpu\") else np.array(y_batch)\n",
        "y_pred = np.array(preds)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(y_true, y_pred, s=12)\n",
        "plt.xlabel(\"True Maximum Wind\")\n",
        "plt.ylabel(\"Predicted Maximum Wind\")\n",
        "plt.title(\"True vs Predicted Wind\")\n",
        "\n",
        "# y = x reference line\n",
        "mn = min(y_true.min(), y_pred.min())\n",
        "mx = max(y_true.max(), y_pred.max())\n",
        "plt.plot([mn, mx], [mn, mx])\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oIQ9l_4EhxT"
      },
      "source": [
        "**First 50 samples (line plot)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShIMtQlrES8n"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "y_true = y_batch.cpu().numpy() if hasattr(y_batch, \"cpu\") else np.array(y_batch)\n",
        "y_pred = np.array(preds)\n",
        "\n",
        "n = 50\n",
        "plt.figure()\n",
        "plt.plot(y_true[:n], label=\"True\")\n",
        "plt.plot(y_pred[:n], label=\"Predicted\")\n",
        "plt.xlabel(\"Sample index (in batch)\")\n",
        "plt.ylabel(\"Maximum Wind\")\n",
        "plt.title(\"Batch Slice: True vs Predicted\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyAFfVNsIDkk"
      },
      "source": [
        "**Absolute Error Distribution (Histogram)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtlOJxYoElH6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "abs_error = np.abs(y_pred - y_true)\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(abs_error, bins=30)\n",
        "plt.xlabel(\"Absolute Error (kt)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Absolute Error Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsfrBaXBIGYl"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.scatter(y_true, abs_error, s=12)\n",
        "plt.xlabel(\"True Maximum Wind (kt)\")\n",
        "plt.ylabel(\"Absolute Error (kt)\")\n",
        "plt.title(\"Error vs True Wind Speed\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-e2ADMTIMq3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def wind_to_category(w):\n",
        "    if w < 34: return 0\n",
        "    if w < 64: return 1\n",
        "    if w < 83: return 2\n",
        "    if w < 96: return 3\n",
        "    if w < 113: return 4\n",
        "    if w < 137: return 5\n",
        "    return 6\n",
        "\n",
        "y_true_cat = np.array([wind_to_category(w) for w in y_true])\n",
        "y_pred_cat = np.array([wind_to_category(w) for w in y_pred])\n",
        "\n",
        "cm = confusion_matrix(y_true_cat, y_pred_cat)\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot()\n",
        "plt.title(\"Wind Category Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spwFa8b5IcL9"
      },
      "source": [
        "##'**Compare Neural vs Neuro-Symbolic curves**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMT3euDAIk-v"
      },
      "source": [
        "**1A) Neural-only model (baseline)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrEFdtvOIQz_"
      },
      "outputs": [],
      "source": [
        "model_neural = GRURegressor(input_dim=input_dim, hidden_dim=128)\n",
        "\n",
        "model_neural = train_model(\n",
        "    model_neural,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=8,\n",
        "    lr=1e-3,\n",
        "    lambda_sym=0.0   # <<< IMPORTANT: no symbolic loss\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z_g7p_vJbS8"
      },
      "source": [
        "**1B) Neuro-symbolic model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_uFhH_IIsPh"
      },
      "outputs": [],
      "source": [
        "model_neurosym = GRURegressor(input_dim=input_dim, hidden_dim=128)\n",
        "\n",
        "model_neurosym = train_model(\n",
        "    model_neurosym,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=8,\n",
        "    lr=1e-3,\n",
        "    lambda_sym=1.0   # <<< symbolic constraints ON\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNSAITalKILg"
      },
      "source": [
        "**Step 2: Get predictions from BOTH models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txD-uYaRJeza"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def collect_predictions(model, loader):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y, metas in loader:\n",
        "            x = x.to(device)\n",
        "            y_hat = model(x).cpu().numpy()\n",
        "            preds.append(y_hat)\n",
        "            trues.append(y.numpy())\n",
        "\n",
        "    return np.concatenate(trues), np.concatenate(preds)\n",
        "\n",
        "y_true, y_pred_neural   = collect_predictions(model_neural, val_loader)\n",
        "_,      y_pred_neurosym = collect_predictions(model_neurosym, val_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TWYR8kcKmF4"
      },
      "source": [
        "**Step 3: Accuracy-within-tolerance CURVE (most important)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUnYoG9iKM2j"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "abs_err_neural   = np.abs(y_pred_neural - y_true)\n",
        "abs_err_neurosym = np.abs(y_pred_neurosym - y_true)\n",
        "\n",
        "tols = np.arange(0, 31, 1)\n",
        "\n",
        "acc_neural   = [np.mean(abs_err_neural   <= t) for t in tols]\n",
        "acc_neurosym = [np.mean(abs_err_neurosym <= t) for t in tols]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(tols, acc_neural,   label=\"Neural only\")\n",
        "plt.plot(tols, acc_neurosym, label=\"Neuro-symbolic\")\n",
        "plt.xlabel(\"Tolerance (± kt)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs Tolerance: Neural vs Neuro-Symbolic\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXAh663_KxQ_"
      },
      "source": [
        "**Step 5: Residual comparison (bias check)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH70fV_IKo3T"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.scatter(y_true, y_pred_neural - y_true, s=10, label=\"Neural only\")\n",
        "plt.scatter(y_true, y_pred_neurosym - y_true, s=10, label=\"Neuro-symbolic\")\n",
        "plt.axhline(0)\n",
        "plt.xlabel(\"True Wind (kt)\")\n",
        "plt.ylabel(\"Residual (Pred − True)\")\n",
        "plt.title(\"Residuals: Neural vs Neuro-Symbolic\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7Iwjcu3KvCe"
      },
      "outputs": [],
      "source": [
        "def summarize(y_true, y_pred, name):\n",
        "    mae = np.mean(np.abs(y_pred - y_true))\n",
        "    rmse = np.sqrt(np.mean((y_pred - y_true)**2))\n",
        "    acc10 = np.mean(np.abs(y_pred - y_true) <= 10)\n",
        "    print(f\"{name}: MAE={mae:.2f}, RMSE={rmse:.2f}, Acc±10kt={acc10*100:.2f}%\")\n",
        "\n",
        "summarize(y_true, y_pred_neural,   \"Neural only\")\n",
        "summarize(y_true, y_pred_neurosym, \"Neuro-symbolic\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiTzvkV409zo"
      },
      "source": [
        "##**GIS Block: Tracks + Risk Grid Map (Atlantic/Pacific)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB5QP_hh29-w"
      },
      "source": [
        "**1) Install + imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZOK0lXL36mD"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas shapely folium\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, LineString, box\n",
        "import folium\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aud4shg83SeM"
      },
      "source": [
        "**2) Load Kaggle files + parse lat/lon**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrBex3-13Vtw"
      },
      "outputs": [],
      "source": [
        "def parse_latlon(v):\n",
        "    # handles formats like '28.0N', '94.8W'\n",
        "    if pd.isna(v):\n",
        "        return np.nan\n",
        "    s = str(v).strip()\n",
        "    if len(s) < 2:\n",
        "        return np.nan\n",
        "    hemi = s[-1].upper()\n",
        "    try:\n",
        "        val = float(s[:-1])\n",
        "    except:\n",
        "        return np.nan\n",
        "    if hemi in (\"S\", \"W\"):\n",
        "        val = -val\n",
        "    return val\n",
        "\n",
        "def load_kaggle_hurdat(atl_path: str, pac_path: str) -> pd.DataFrame:\n",
        "    atl = pd.read_csv(atl_path)\n",
        "    pac = pd.read_csv(pac_path)\n",
        "    atl[\"BASIN\"] = \"ATL\"\n",
        "    pac[\"BASIN\"] = \"PAC\"\n",
        "    df = pd.concat([atl, pac], ignore_index=True)\n",
        "\n",
        "    df = df.replace(-999, np.nan)\n",
        "\n",
        "    df[\"lat\"] = df[\"Latitude\"].apply(parse_latlon)\n",
        "    df[\"lon\"] = df[\"Longitude\"].apply(parse_latlon)\n",
        "\n",
        "    df[\"Date\"] = pd.to_numeric(df[\"Date\"], errors=\"coerce\")\n",
        "    df[\"Time\"] = pd.to_numeric(df[\"Time\"], errors=\"coerce\")\n",
        "    df[\"dt_key\"] = df[\"Date\"] * 10000 + df[\"Time\"]\n",
        "\n",
        "    df[\"Maximum Wind\"] = pd.to_numeric(df[\"Maximum Wind\"], errors=\"coerce\")\n",
        "\n",
        "    # keep usable rows\n",
        "    df[\"BASIN\"] = df[\"BASIN\"].fillna(\"UNK\").astype(str).str.strip()\n",
        "    df[\"ID\"] = df[\"ID\"].astype(str).str.strip()\n",
        "    df = df.dropna(subset=[\"ID\", \"dt_key\", \"lat\", \"lon\", \"Maximum Wind\"]).reset_index(drop=True)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbltuR9Z3ZR3"
      },
      "source": [
        "**3) Convert to GeoDataFrame (GIS points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3m-o7C53XL6"
      },
      "outputs": [],
      "source": [
        "def to_points_gdf(df: pd.DataFrame) -> gpd.GeoDataFrame:\n",
        "    d = df.copy()\n",
        "    d[\"geometry\"] = [Point(float(x), float(y)) for x, y in zip(d[\"lon\"], d[\"lat\"])]\n",
        "    return gpd.GeoDataFrame(d, geometry=\"geometry\", crs=\"EPSG:4326\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3PcApYN3e-y"
      },
      "source": [
        "**4) Build track lines (GIS polylines per storm)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd-DYtjD3cJ8"
      },
      "outputs": [],
      "source": [
        "def build_track_lines(points_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
        "    rows = []\n",
        "    for storm_id, g in points_gdf.sort_values(\"dt_key\").groupby(\"ID\"):\n",
        "        if len(g) < 2:\n",
        "            continue\n",
        "        line = LineString(list(g.geometry.values))\n",
        "        rows.append({\n",
        "            \"ID\": storm_id,\n",
        "            \"BASIN\": str(g[\"BASIN\"].iloc[0]),\n",
        "            \"n_points\": int(len(g)),\n",
        "            \"max_wind\": float(np.nanmax(g[\"Maximum Wind\"].values)),\n",
        "            \"geometry\": line\n",
        "        })\n",
        "    return gpd.GeoDataFrame(rows, geometry=\"geometry\", crs=\"EPSG:4326\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bizlHgOs3lP1"
      },
      "source": [
        "**5) Build a GIS “risk grid” (max wind per 1° cell)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Rfgi9oy3i_T"
      },
      "outputs": [],
      "source": [
        "def make_risk_grid(points_gdf: gpd.GeoDataFrame, cell_deg: float = 1.0) -> gpd.GeoDataFrame:\n",
        "    d = points_gdf.copy()\n",
        "\n",
        "    # cell id by flooring lon/lat to grid\n",
        "    d[\"gx\"] = np.floor(d.geometry.x / cell_deg) * cell_deg\n",
        "    d[\"gy\"] = np.floor(d.geometry.y / cell_deg) * cell_deg\n",
        "\n",
        "    agg = d.groupby([\"gx\", \"gy\"], as_index=False)[\"Maximum Wind\"].max()\n",
        "    polys = [box(x, y, x + cell_deg, y + cell_deg) for x, y in zip(agg[\"gx\"], agg[\"gy\"])]\n",
        "\n",
        "    grid = gpd.GeoDataFrame(agg, geometry=polys, crs=\"EPSG:4326\")\n",
        "    grid = grid.rename(columns={\"Maximum Wind\": \"wind_max\"})\n",
        "    return grid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-AIyKYW3qcE"
      },
      "source": [
        "**6) Interactive GIS map (Folium) + save HTML**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_qeVWDa3oHo"
      },
      "outputs": [],
      "source": [
        "def folium_gis_map(points_gdf, lines_gdf, grid_gdf, out_html=\"hurdat_gis_map.html\",\n",
        "                   max_tracks=200, max_points=5000):\n",
        "\n",
        "    center = [float(points_gdf[\"lat\"].mean()), float(points_gdf[\"lon\"].mean())]\n",
        "    m = folium.Map(location=center, zoom_start=3, tiles=\"CartoDB positron\")\n",
        "\n",
        "    # Risk grid layer\n",
        "    if grid_gdf is not None and len(grid_gdf) > 0:\n",
        "        folium.GeoJson(\n",
        "            grid_gdf.to_json(),\n",
        "            name=\"Risk Grid (max wind)\",\n",
        "            style_function=lambda feat: {\"weight\": 0.2, \"fillOpacity\": 0.35},\n",
        "            tooltip=folium.GeoJsonTooltip(fields=[\"wind_max\"], aliases=[\"Max wind:\"])\n",
        "        ).add_to(m)\n",
        "\n",
        "    # Tracks layer (subset)\n",
        "    if lines_gdf is not None and len(lines_gdf) > 0:\n",
        "        sample_lines = lines_gdf.sample(min(len(lines_gdf), max_tracks), random_state=42)\n",
        "        for _, r in sample_lines.iterrows():\n",
        "            coords = [(y, x) for x, y in list(r.geometry.coords)]  # lat,lon\n",
        "            folium.PolyLine(coords, weight=2, opacity=0.7,\n",
        "                            tooltip=f\"ID={r['ID']} | Basin={r['BASIN']} | MaxWind={r['max_wind']}\").add_to(m)\n",
        "\n",
        "    # Points layer (subset)\n",
        "    sample_pts = points_gdf.sample(min(len(points_gdf), max_points), random_state=42)\n",
        "    for _, r in sample_pts.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[float(r[\"lat\"]), float(r[\"lon\"])],\n",
        "            radius=2,\n",
        "            tooltip=f\"Wind={float(r['Maximum Wind'])}\",\n",
        "            fill=True,\n",
        "            fill_opacity=0.7\n",
        "        ).add_to(m)\n",
        "\n",
        "    folium.LayerControl().add_to(m)\n",
        "    m.save(out_html)\n",
        "    return m, out_html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaJfNWy78faT"
      },
      "outputs": [],
      "source": [
        "def wind_category(w):\n",
        "    if w < 34:\n",
        "        return \"TD\"\n",
        "    elif w < 64:\n",
        "        return \"TS\"\n",
        "    elif w < 83:\n",
        "        return \"C1\"\n",
        "    elif w < 96:\n",
        "        return \"C2\"\n",
        "    elif w < 113:\n",
        "        return \"C3\"\n",
        "    elif w < 137:\n",
        "        return \"C4\"\n",
        "    else:\n",
        "        return \"C5\"\n",
        "\n",
        "\n",
        "def wind_color(w):\n",
        "    if w < 34:\n",
        "        return \"blue\"\n",
        "    elif w < 64:\n",
        "        return \"green\"\n",
        "    elif w < 83:\n",
        "        return \"yellow\"\n",
        "    elif w < 96:\n",
        "        return \"orange\"\n",
        "    elif w < 113:\n",
        "        return \"red\"\n",
        "    elif w < 137:\n",
        "        return \"darkred\"\n",
        "    else:\n",
        "        return \"purple\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2aEccvk8hUW"
      },
      "outputs": [],
      "source": [
        "df_gis = load_kaggle_hurdat(\"/content/atlantic.csv\", \"/content/pacific.csv\")\n",
        "points_gdf = to_points_gdf(df_gis)\n",
        "lines_gdf = build_track_lines(points_gdf)\n",
        "\n",
        "#Add wind type + color to GeoDataFrame\n",
        "points_gdf[\"wind_type\"] = points_gdf[\"Maximum Wind\"].apply(wind_category)\n",
        "points_gdf[\"wind_color\"] = points_gdf[\"Maximum Wind\"].apply(wind_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc2yINdq8o-E"
      },
      "outputs": [],
      "source": [
        "def folium_wind_category_map(points_gdf, lines_gdf, out_html=\"wind_category_map.html\",\n",
        "                             max_points=6000, max_tracks=200):\n",
        "\n",
        "    center = [float(points_gdf[\"lat\"].mean()), float(points_gdf[\"lon\"].mean())]\n",
        "    m = folium.Map(location=center, zoom_start=3, tiles=\"CartoDB positron\")\n",
        "\n",
        "    # ---- Storm tracks (neutral color) ----\n",
        "    if lines_gdf is not None:\n",
        "        for _, r in lines_gdf.sample(min(len(lines_gdf), max_tracks), random_state=42).iterrows():\n",
        "            coords = [(y, x) for x, y in list(r.geometry.coords)]\n",
        "            folium.PolyLine(\n",
        "                coords,\n",
        "                color=\"black\",\n",
        "                weight=2,\n",
        "                opacity=0.6,\n",
        "                tooltip=f\"Storm ID: {r['ID']}\"\n",
        "            ).add_to(m)\n",
        "\n",
        "    # ---- Colored wind points ----\n",
        "    pts = points_gdf.sample(min(len(points_gdf), max_points), random_state=42)\n",
        "    for _, r in pts.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[float(r[\"lat\"]), float(r[\"lon\"])],\n",
        "            radius=3,\n",
        "            color=r[\"wind_color\"],\n",
        "            fill=True,\n",
        "            fill_color=r[\"wind_color\"],\n",
        "            fill_opacity=0.8,\n",
        "            tooltip=f\"Wind: {int(r['Maximum Wind'])} kt ({r['wind_type']})\"\n",
        "        ).add_to(m)\n",
        "\n",
        "    # ---- Legend (HTML) ----\n",
        "    legend_html = \"\"\"\n",
        "    <div style=\"\n",
        "        position: fixed;\n",
        "        bottom: 50px; left: 50px; width: 210px;\n",
        "        background-color: white; z-index:9999;\n",
        "        padding: 10px; border: 2px solid grey;\n",
        "        font-size: 13px;\">\n",
        "    <b>Wind Categories</b><br>\n",
        "    <i style=\"color:blue\">●</i> TD (&lt;34 kt)<br>\n",
        "    <i style=\"color:green\">●</i> TS (34–63 kt)<br>\n",
        "    <i style=\"color:yellow\">●</i> Cat 1 (64–82 kt)<br>\n",
        "    <i style=\"color:orange\">●</i> Cat 2 (83–95 kt)<br>\n",
        "    <i style=\"color:red\">●</i> Cat 3 (96–112 kt)<br>\n",
        "    <i style=\"color:darkred\">●</i> Cat 4 (113–136 kt)<br>\n",
        "    <i style=\"color:purple\">●</i> Cat 5 (≥137 kt)\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    m.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "    m.save(out_html)\n",
        "    return m, out_html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2UKxoz780nt"
      },
      "outputs": [],
      "source": [
        "m, html_file = folium_wind_category_map(\n",
        "    points_gdf,\n",
        "    lines_gdf,\n",
        "    out_html=\"hurricane_wind_categories.html\"\n",
        ")\n",
        "\n",
        "print(\"Saved GIS wind-category map to:\", html_file)\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_waaCwwI83bI"
      },
      "outputs": [],
      "source": [
        "AREAS = {\n",
        "    \"Bay of Bengal\":  {\"lat_min\": 5,  \"lat_max\": 25, \"lon_min\": 75,  \"lon_max\": 100},\n",
        "    \"Arabian Sea\":    {\"lat_min\": 5,  \"lat_max\": 25, \"lon_min\": 55,  \"lon_max\": 75},\n",
        "    \"India\":          {\"lat_min\": 6,  \"lat_max\": 36, \"lon_min\": 68,  \"lon_max\": 98},\n",
        "    \"USA East Coast\": {\"lat_min\": 20, \"lat_max\": 50, \"lon_min\": -90, \"lon_max\": -60},\n",
        "    \"Caribbean\":      {\"lat_min\": 10, \"lat_max\": 30, \"lon_min\": -90, \"lon_max\": -60},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou33BKPm9skA"
      },
      "outputs": [],
      "source": [
        "def filter_by_bbox(points_gdf, lat_min, lat_max, lon_min, lon_max):\n",
        "    return points_gdf[\n",
        "        (points_gdf[\"lat\"] >= lat_min) & (points_gdf[\"lat\"] <= lat_max) &\n",
        "        (points_gdf[\"lon\"] >= lon_min) & (points_gdf[\"lon\"] <= lon_max)\n",
        "    ].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w6rZr5M9uQu"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "\n",
        "def area_map(points_gdf, area_name=\"USA East Coast\", out_html=\"area_map.html\", max_points=8000):\n",
        "    area = AREAS[area_name]\n",
        "\n",
        "    sub = filter_by_bbox(points_gdf, **area)\n",
        "    if len(sub) == 0:\n",
        "        raise ValueError(f\"No points found in selected area: {area_name}\")\n",
        "\n",
        "    center = [float((area[\"lat_min\"] + area[\"lat_max\"]) / 2),\n",
        "              float((area[\"lon_min\"] + area[\"lon_max\"]) / 2)]\n",
        "\n",
        "    m = folium.Map(location=center, zoom_start=5, tiles=\"CartoDB positron\")\n",
        "\n",
        "    # add points (colored)\n",
        "    pts = sub.sample(min(len(sub), max_points), random_state=42)\n",
        "    for _, r in pts.iterrows():\n",
        "        w = float(r[\"Maximum Wind\"])\n",
        "        folium.CircleMarker(\n",
        "            location=[float(r[\"lat\"]), float(r[\"lon\"])],\n",
        "            radius=3,\n",
        "            color=wind_color(w),\n",
        "            fill=True,\n",
        "            fill_color=wind_color(w),\n",
        "            fill_opacity=0.85,\n",
        "            tooltip=f\"Wind: {int(w)} kt ({wind_category(w)}) | ID: {r['ID']}\"\n",
        "        ).add_to(m)\n",
        "\n",
        "    # draw bounding box on map\n",
        "    bounds = [[area[\"lat_min\"], area[\"lon_min\"]], [area[\"lat_max\"], area[\"lon_max\"]]]\n",
        "    folium.Rectangle(bounds=bounds, color=\"black\", fill=False).add_to(m)\n",
        "\n",
        "    m.save(out_html)\n",
        "    return m, out_html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJpuGWIE9wIj"
      },
      "outputs": [],
      "source": [
        "m, html_file = area_map(points_gdf, area_name=\"USA East Coast\", out_html=\"usa_east_coast_map.html\")\n",
        "print(\"Saved:\", html_file)\n",
        "m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4OGDB0i-_Dr"
      },
      "source": [
        "**Carribean**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ynf5Ihe9zJH"
      },
      "outputs": [],
      "source": [
        "CARIBBEAN = {\n",
        "    \"lat_min\": 10,\n",
        "    \"lat_max\": 30,\n",
        "    \"lon_min\": -90,\n",
        "    \"lon_max\": -60\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mghfjbvz_DEZ"
      },
      "outputs": [],
      "source": [
        "def wind_category(w):\n",
        "    if w < 34: return \"TD\"\n",
        "    elif w < 64: return \"TS\"\n",
        "    elif w < 83: return \"C1\"\n",
        "    elif w < 96: return \"C2\"\n",
        "    elif w < 113: return \"C3\"\n",
        "    elif w < 137: return \"C4\"\n",
        "    else: return \"C5\"\n",
        "\n",
        "def wind_color(w):\n",
        "    if w < 34: return \"blue\"\n",
        "    elif w < 64: return \"green\"\n",
        "    elif w < 83: return \"yellow\"\n",
        "    elif w < 96: return \"orange\"\n",
        "    elif w < 113: return \"red\"\n",
        "    elif w < 137: return \"darkred\"\n",
        "    else: return \"purple\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCj7ahvqBaBX"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "import folium\n",
        "\n",
        "def show_folium_2x2(maps, titles=None, width=\"48%\", height=360):\n",
        "    \"\"\"\n",
        "    maps: list of 4 folium.Map objects\n",
        "    titles: list of 4 strings (optional)\n",
        "    \"\"\"\n",
        "    if titles is None:\n",
        "        titles = [\"Map 1\", \"Map 2\", \"Map 3\", \"Map 4\"]\n",
        "\n",
        "    html_blocks = []\n",
        "    for m, t in zip(maps, titles):\n",
        "        m_html = m.get_root().render()\n",
        "        block = f\"\"\"\n",
        "        <div style=\"width:{width}; padding:6px; box-sizing:border-box;\">\n",
        "            <div style=\"font-weight:600; margin:4px 0 6px 2px;\">{t}</div>\n",
        "            <div style=\"border:1px solid #ddd; border-radius:6px; overflow:hidden;\">\n",
        "                {m_html}\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        html_blocks.append(block)\n",
        "\n",
        "    grid_html = f\"\"\"\n",
        "    <div style=\"display:flex; flex-wrap:wrap; justify-content:space-between;\">\n",
        "        {''.join(html_blocks)}\n",
        "    </div>\n",
        "    <style>\n",
        "      .folium-map {{ height: {height}px !important; }}\n",
        "    </style>\n",
        "    \"\"\"\n",
        "\n",
        "    display(HTML(grid_html))\n",
        "\n",
        "# --- Start of moved code from r6Ojbix4Eej6 to define Atlantic Region maps ---\n",
        "# --- Atlantic bounding box (Caribbean + Gulf + MDR) ---\n",
        "ATLANTIC = {\n",
        "    \"lat_min\": 5,\n",
        "    \"lat_max\": 40,\n",
        "    \"lon_min\": -100,\n",
        "    \"lon_max\": -10\n",
        "}\n",
        "\n",
        "# --- Wind category ---\n",
        "def wind_category(w):\n",
        "    if w < 34: return \"TD\"\n",
        "    elif w < 64: return \"TS\"\n",
        "    elif w < 83: return \"C1\"\n",
        "    elif w < 96: return \"C2\"\n",
        "    elif w < 113: return \"C3\"\n",
        "    elif w < 137: return \"C4\"\n",
        "    else: return \"C5\"\n",
        "\n",
        "def wind_color(cat):\n",
        "    return {\"TD\":\"blue\", \"TS\":\"green\", \"C1\":\"yellow\", \"C2\":\"orange\"}.get(cat, \"gray\")\n",
        "\n",
        "# --- Filter Atlantic points ---\n",
        "atl_pts = points_gdf[\n",
        "    (points_gdf[\"lat\"] >= ATLANTIC[\"lat_min\"]) &\n",
        "    (points_gdf[\"lat\"] <= ATLANTIC[\"lat_max\"]) &\n",
        "    (points_gdf[\"lon\"] >= ATLANTIC[\"lon_min\"]) &\n",
        "    (points_gdf[\"lon\"] <= ATLANTIC[\"lon_max\"])\n",
        "].copy()\n",
        "\n",
        "atl_pts[\"wind_type\"] = atl_pts[\"Maximum Wind\"].apply(lambda x: wind_category(float(x)))\n",
        "\n",
        "# --- Map builder (one category) ---\n",
        "def make_atlantic_map(df, category, title, max_points=6000):\n",
        "    center_lat = (ATLANTIC[\"lat_min\"] + ATLANTIC[\"lat_max\"]) / 2\n",
        "    center_lon = (ATLANTIC[\"lon_min\"] + ATLANTIC[\"lon_max\"]) / 2\n",
        "\n",
        "    m = folium.Map(location=[center_lat, center_lon], zoom_start=4, tiles=\"CartoDB positron\")\n",
        "\n",
        "    sub = df[df[\"wind_type\"] == category].copy()\n",
        "    print(f\"{category} points:\", len(sub))\n",
        "\n",
        "    if len(sub) == 0:\n",
        "        return m\n",
        "\n",
        "    sub = sub.sample(min(len(sub), max_points), random_state=42)\n",
        "\n",
        "    for _, r in sub.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[float(r[\"lat\"]), float(r[\"lon\"])],\n",
        "            radius=3,\n",
        "            color=wind_color(category),\n",
        "            fill=True,\n",
        "            fill_color=wind_color(category),\n",
        "            fill_opacity=0.85,\n",
        "            tooltip=f\"{title} | Wind {int(r['Maximum Wind'])} kt\"\n",
        "        ).add_to(m)\n",
        "\n",
        "    return m\n",
        "\n",
        "# --- Create 4 Atlantic maps ---\n",
        "map_TD = make_atlantic_map(atl_pts, \"TD\", \"Atlantic: Tropical Depression\")\n",
        "map_TS = make_atlantic_map(atl_pts, \"TS\", \"Atlantic: Tropical Storm\")\n",
        "map_C1 = make_atlantic_map(atl_pts, \"C1\", \"Atlantic: Category 1 Hurricane\")\n",
        "map_C2 = make_atlantic_map(atl_pts, \"C2\", \"Atlantic: Category 2 Hurricane\")\n",
        "# --- End of moved code ---\n",
        "\n",
        "# --- Show your 4 category maps in a 2×2 grid ---\n",
        "show_folium_2x2(\n",
        "    maps=[map_TD, map_TS, map_C1, map_C2],\n",
        "    titles=[\"TD (Tropical Depression)\", \"TS (Tropical Storm)\", \"C1 (Category 1)\", \"C2 (Category 2)\"],\n",
        "    height=360\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az07Y1D0EY-m"
      },
      "source": [
        "**Atlantic Region**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6Ojbix4Eej6"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# --- Atlantic bounding box (Caribbean + Gulf + MDR) ---\n",
        "ATLANTIC = {\n",
        "    \"lat_min\": 5,\n",
        "    \"lat_max\": 40,\n",
        "    \"lon_min\": -100,\n",
        "    \"lon_max\": -10\n",
        "}\n",
        "\n",
        "# --- Wind category ---\n",
        "def wind_category(w):\n",
        "    if w < 34: return \"TD\"\n",
        "    elif w < 64: return \"TS\"\n",
        "    elif w < 83: return \"C1\"\n",
        "    elif w < 96: return \"C2\"\n",
        "    elif w < 113: return \"C3\"\n",
        "    elif w < 137: return \"C4\"\n",
        "    else: return \"C5\"\n",
        "\n",
        "def wind_color(cat):\n",
        "    return {\"TD\":\"blue\", \"TS\":\"green\", \"C1\":\"yellow\", \"C2\":\"orange\"}.get(cat, \"gray\")\n",
        "\n",
        "# --- Filter Atlantic points ---\n",
        "atl_pts = points_gdf[\n",
        "    (points_gdf[\"lat\"] >= ATLANTIC[\"lat_min\"]) &\n",
        "    (points_gdf[\"lat\"] <= ATLANTIC[\"lat_max\"]) &\n",
        "    (points_gdf[\"lon\"] >= ATLANTIC[\"lon_min\"]) &\n",
        "    (points_gdf[\"lon\"] <= ATLANTIC[\"lon_max\"])\n",
        "].copy()\n",
        "\n",
        "atl_pts[\"wind_type\"] = atl_pts[\"Maximum Wind\"].apply(lambda x: wind_category(float(x)))\n",
        "\n",
        "# --- Map builder (one category) ---\n",
        "def make_atlantic_map(df, category, title, max_points=6000):\n",
        "    center_lat = (ATLANTIC[\"lat_min\"] + ATLANTIC[\"lat_max\"]) / 2\n",
        "    center_lon = (ATLANTIC[\"lon_min\"] + ATLANTIC[\"lon_max\"]) / 2\n",
        "\n",
        "    m = folium.Map(location=[center_lat, center_lon], zoom_start=4, tiles=\"CartoDB positron\")\n",
        "\n",
        "    sub = df[df[\"wind_type\"] == category].copy()\n",
        "    print(f\"{category} points:\", len(sub))\n",
        "\n",
        "    if len(sub) == 0:\n",
        "        return m\n",
        "\n",
        "    sub = sub.sample(min(len(sub), max_points), random_state=42)\n",
        "\n",
        "    for _, r in sub.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[float(r[\"lat\"]), float(r[\"lon\"])],\n",
        "            radius=3,\n",
        "            color=wind_color(category),\n",
        "            fill=True,\n",
        "            fill_color=wind_color(category),\n",
        "            fill_opacity=0.85,\n",
        "            tooltip=f\"{title} | Wind {int(r['Maximum Wind'])} kt\"\n",
        "        ).add_to(m)\n",
        "\n",
        "    return m\n",
        "\n",
        "# --- Create 4 Atlantic maps ---\n",
        "map_TD = make_atlantic_map(atl_pts, \"TD\", \"Atlantic: Tropical Depression\")\n",
        "map_TS = make_atlantic_map(atl_pts, \"TS\", \"Atlantic: Tropical Storm\")\n",
        "map_C1 = make_atlantic_map(atl_pts, \"C1\", \"Atlantic: Category 1 Hurricane\")\n",
        "map_C2 = make_atlantic_map(atl_pts, \"C2\", \"Atlantic: Category 2 Hurricane\")\n",
        "\n",
        "# --- 2×2 layout display ---\n",
        "def show_2x2(maps, titles, height=360):\n",
        "    blocks = \"\"\n",
        "    for m, t in zip(maps, titles):\n",
        "        blocks += f\"\"\"\n",
        "        <div style=\"width:48%; padding:6px; box-sizing:border-box;\">\n",
        "            <b>{t}</b>\n",
        "            {m.get_root().render()}\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    display(HTML(f\"\"\"\n",
        "    <div style=\"display:flex; flex-wrap:wrap; justify-content:space-between;\">\n",
        "        {blocks}\n",
        "    </div>\n",
        "    <style>.folium-map {{ height:{height}px !important; }}</style>\n",
        "    \"\"\"))\n",
        "\n",
        "show_2x2([map_TD, map_TS, map_C1, map_C2], [\"TD\", \"TS\", \"C1\", \"C2\"], height=360)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJp-tQE7Dbnf"
      },
      "source": [
        "**Pacific Region**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrqEDmnCDei2"
      },
      "outputs": [],
      "source": [
        "PACIFIC = {\n",
        "    \"lat_min\": 0,\n",
        "    \"lat_max\": 30,\n",
        "    \"lon_min\": -140,\n",
        "    \"lon_max\": -90\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8127c1tDkCA"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# --- Wind category ---\n",
        "def wind_category(w):\n",
        "    if w < 34: return \"TD\"\n",
        "    elif w < 64: return \"TS\"\n",
        "    elif w < 83: return \"C1\"\n",
        "    elif w < 96: return \"C2\"\n",
        "    elif w < 113: return \"C3\"\n",
        "    elif w < 137: return \"C4\"\n",
        "    else: return \"C5\"\n",
        "\n",
        "def wind_color(cat):\n",
        "    return {\"TD\":\"blue\", \"TS\":\"green\", \"C1\":\"yellow\", \"C2\":\"orange\"}.get(cat, \"gray\")\n",
        "\n",
        "# --- Filter Pacific points ---\n",
        "pacific_pts = points_gdf[\n",
        "    (points_gdf[\"lat\"] >= PACIFIC[\"lat_min\"]) &\n",
        "    (points_gdf[\"lat\"] <= PACIFIC[\"lat_max\"]) &\n",
        "    (points_gdf[\"lon\"] >= PACIFIC[\"lon_min\"]) &\n",
        "    (points_gdf[\"lon\"] <= PACIFIC[\"lon_max\"])\n",
        "].copy()\n",
        "\n",
        "pacific_pts[\"wind_type\"] = pacific_pts[\"Maximum Wind\"].apply(lambda x: wind_category(float(x)))\n",
        "\n",
        "# --- Map builder for one category ---\n",
        "def make_pacific_map(df, category, title, max_points=6000):\n",
        "    center_lat = (PACIFIC[\"lat_min\"] + PACIFIC[\"lat_max\"]) / 2\n",
        "    center_lon = (PACIFIC[\"lon_min\"] + PACIFIC[\"lon_max\"]) / 2\n",
        "\n",
        "    m = folium.Map(\n",
        "        location=[center_lat, center_lon],\n",
        "        zoom_start=4,\n",
        "        tiles=\"CartoDB positron\"\n",
        "    )\n",
        "\n",
        "    sub = df[df[\"wind_type\"] == category]\n",
        "    print(f\"{category} points:\", len(sub))\n",
        "\n",
        "    sub = sub.sample(min(len(sub), max_points), random_state=42)\n",
        "\n",
        "    for _, r in sub.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[float(r[\"lat\"]), float(r[\"lon\"])],\n",
        "            radius=3,\n",
        "            color=wind_color(category),\n",
        "            fill=True,\n",
        "            fill_color=wind_color(category),\n",
        "            fill_opacity=0.85,\n",
        "            tooltip=f\"{title} | Wind {int(r['Maximum Wind'])} kt\"\n",
        "        ).add_to(m)\n",
        "\n",
        "    return m\n",
        "\n",
        "# --- Create maps ---\n",
        "map_TD = make_pacific_map(pacific_pts, \"TD\", \"Pacific: Tropical Depression\")\n",
        "map_TS = make_pacific_map(pacific_pts, \"TS\", \"Pacific: Tropical Storm\")\n",
        "map_C1 = make_pacific_map(pacific_pts, \"C1\", \"Pacific: Category 1 Hurricane\")\n",
        "map_C2 = make_pacific_map(pacific_pts, \"C2\", \"Pacific: Category 2 Hurricane\")\n",
        "\n",
        "# --- 2×2 layout ---\n",
        "def show_2x2(maps, titles, height=360):\n",
        "    html = \"\"\n",
        "    for m, t in zip(maps, titles):\n",
        "        html += f\"\"\"\n",
        "        <div style=\"width:48%; padding:6px;\">\n",
        "            <b>{t}</b>\n",
        "            {m.get_root().render()}\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    display(HTML(f\"\"\"\n",
        "    <div style=\"display:flex; flex-wrap:wrap; justify-content:space-between;\">\n",
        "        {html}\n",
        "    </div>\n",
        "    <style>.folium-map {{ height:{height}px !important; }}</style>\n",
        "    \"\"\"))\n",
        "\n",
        "show_2x2(\n",
        "    [map_TD, map_TS, map_C1, map_C2],\n",
        "    [\"TD\", \"TS\", \"C1\", \"C2\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lvY4i0DsvS9h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}